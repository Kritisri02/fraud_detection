# ðŸ“Œ Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# ðŸ“Œ Step 2: Load Dataset
df = pd.read_csv("data/fraud.csv")
print("Data Shape:", df.shape)
df.head()

# ðŸ“Œ Step 3: Check Missing Values
print(df.isnull().sum())

# ðŸ“Œ Step 4: Detect Outliers (Optional - Quick Summary)
plt.figure(figsize=(10, 5))
sns.boxplot(data=df.select_dtypes(include=np.number))
plt.title("Boxplot to check for outliers")
plt.show()

# ðŸ“Œ Step 5: Multicollinearity (VIF)
X = df.select_dtypes(include=np.number).drop(columns=['is_fraud'])
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)

# ðŸ“Œ Step 6: Class Imbalance Check
sns.countplot(x='is_fraud', data=df)
plt.title("Class Distribution")
plt.show()

# ðŸ“Œ Step 7: Feature Engineering (if needed)
# Example: Creating new features
# df['transaction_hour'] = pd.to_datetime(df['timestamp']).dt.hour

# ðŸ“Œ Step 8: Prepare Data for Modeling
X = df.drop("is_fraud", axis=1)
y = df["is_fraud"]

# Encode categorical features if any
X = pd.get_dummies(X, drop_first=True)

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ðŸ“Œ Step 9: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

# ðŸ“Œ Step 10: Handle Imbalance with SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train, y_train)

# ðŸ“Œ Step 11: Model Training
model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
model.fit(X_resampled, y_resampled)

# ðŸ“Œ Step 12: Predictions and Evaluation
y_pred = model.predict(X_test)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

# ROC-AUC
y_probs = model.predict_proba(X_test)[:, 1]
roc_score = roc_auc_score(y_test, y_probs)
print("ROC-AUC Score:", roc_score)

fpr, tpr, _ = roc_curve(y_test, y_probs)
plt.plot(fpr, tpr, label=f"AUC = {roc_score:.2f}")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# ðŸ“Œ Step 13: Feature Importance with SHAP
explainer = shap.Explainer(model)
shap_values = explainer(X_test[:100])  # Sample to reduce compute

shap.plots.bar(shap_values)
shap.summary_plot(shap_values, X_test[:100], plot_type="bar")

# ðŸ“Œ Step 14: Key Findings
print("""
Top Predictive Features:
- Feature 1
- Feature 2
- Feature 3
(Replace with actual SHAP results)

Recommendations:
- Monitor high-value transactions during off-hours
- Use real-time anomaly tracking systems
- Apply geo-location and device fingerprinting
""")
